# -*- coding: utf-8 -*-
"""WiC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OvdhaMUJrpTi_yJZHmib8vylQfRK2-3H
"""

#Author1: Shaivya Chandra sc2914
#Author2: Divya Kumar dk5858
#Author3: Divya Muraidharan dm5643

"""#Author: Shaivya Chandra sc2914
#Author2: Divya Kumar dk5858
"""

from pydub import AudioSegment
from pydub.playback import play
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from scipy.spatial import distance
from tensorflow import keras
from IPython.display import Audio
from gtts import gTTS

from flask_restful import Resource, Api,reqparse
from flask import Flask, request, redirect, jsonify
from werkzeug.utils import secure_filename
from playsound import playsound
import werkzeug
import dill as pickle

import numpy as np
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
MIN_DISTANCE = 130
class visionmaskdetector:
    def checkdir(self):

        for dirname, _, filenames in os.walk('/Users/shaivyachandra/Downloads/Face Mask Dataset'):
            print(dirname , len(filenames))



    #loading haarcascade_frontalface_default.xml
    def loadfacemodel(self):
        #Gathering the XML file for performing Haar cascade frontal face detector
        face_model = cv2.CascadeClassifier('/Users/shaivyachandra/Downloads/archive1/haarcascade_frontalface_default.xml')
        print('done')
        return face_model


    def readimage(self,face_model,image='/Users/shaivyachandra/Downloads/archive/images/maksssksksss244.png'):
        # Read a input image from Face Mask dataset
        face_image = cv2.imread('/Users/shaivyachandra/Downloads/archive/images/maksssksksss244.png')
        # Convert image to grayscale for performing face detection
        face_image = cv2.cvtColor(face_image, cv2.IMREAD_GRAYSCALE)

        # Perform face detection on images and store a list of face locations

        captured_faces = face_model.detectMultiScale(face_image,scaleFactor=1.1, minNeighbors=4) #returns a list of (x,y,w,h) tuples
        # Convert image back to colored version

        colored_image = cv2.cvtColor(face_image, cv2.COLOR_RGB2BGR) #colored output image

        #mark the detected faces
        for (x,y,w,h) in captured_faces:
            cv2.rectangle(colored_image,(x,y),(x+w,y+h),(0,0,255),1)
        plt.figure(figsize=(12,12))
        plt.imshow(colored_image)
        plt.savefig('/Users/shaivyachandra/PycharmProjects/wic/venv/markdetectedfaces.png')
        return captured_faces,colored_image

    def findfaces(self,captured_faces,face_image):
        # If the no. of captured faces is more than 1, try finding distance between those faces using euclidean method
        # If the no. of captured faces is less than 2, display a message
        if len(captured_faces)>=2:
            label = [0 for i in range(len(captured_faces))]
            for i in range(len(captured_faces)-1):
                for j in range(i+1, len(captured_faces)):
                    dist = distance.euclidean(captured_faces[i][:2],captured_faces[j][:2])
                    if MIN_DISTANCE>dist:
                        label[i] = 1
                        label[j] = 1
            new_color_img = cv2.cvtColor(face_image, cv2.COLOR_RGB2BGR)
            for i in range(len(captured_faces)):
                (x,y,w,h) = captured_faces[i]
                if label[i]==1:
                    cv2.rectangle(new_color_img,(x,y),(x+w,y+h),(255,0,0),1)
                else:
                    cv2.rectangle(new_color_img,(x,y),(x+w,y+h),(0,255,0),1)
            plt.figure(figsize=(10,10))
            plt.imshow(new_color_img)

        else:
            print("Less than 2 ppl present near you ")


    def trainthedragon(self):
        # Store path to train, test and validation data
        train_directory = '/Users/shaivyachandra/Downloads/Face Mask Dataset/Train'
        test_directory = '/Users/shaivyachandra/Downloads/Face Mask Dataset/Train'
        val_directory = '/Users/shaivyachandra/Downloads/Face Mask Dataset/Validation'


        # Perform image data augmentation to expand training,validation and test dataset

        train_datagenerator = ImageDataGenerator(rescale=1.0/255, horizontal_flip=True, zoom_range=0.2,shear_range=0.2)
        train_generator = train_datagenerator.flow_from_directory(directory=train_directory,target_size=(128,128),class_mode='categorical',batch_size=32)

        val_datagenerator = ImageDataGenerator(rescale=1.0/255)
        val_generator = val_datagenerator.flow_from_directory(directory=val_directory,target_size=(128,128),class_mode='categorical',batch_size=32)

        test_datagenerator = ImageDataGenerator(rescale=1.0/255)
        test_generator = test_datagenerator.flow_from_directory(directory=test_directory,target_size=(128,128),class_mode='categorical',batch_size=32)

        # Use MobileNetV2 CNN for best Object detection and segmentation
        mobilenet = MobileNetV2(weights='imagenet',include_top=False,input_shape=(128,128,3))
        for layer in mobilenet.layers:
            layer.trainable = False
        model = Sequential()
        model.add(mobilenet)
        model.add(Flatten())
        model.add(Dense(2,activation='sigmoid'))
        model.summary
        # Compile model

        model.compile(optimizer="adam",loss="categorical_crossentropy",metrics ="accuracy")
        # Perform model fitting
        history = model.fit(train_generator,steps_per_epoch=len(train_generator)//32,epochs=20,validation_data=val_generator,validation_steps=len(val_generator)//32)
        # Evaluate model on test data
        print('evaluating')
        model.evaluate(test_generator)
        return model



    def test(self,model,img,captured_faces):
        # Read a random image
        #        sample_mask_img = cv2.imread('/Users/shaivyachandra/Downloads/Face Mask Dataset/Test/WithMask/1555.png')
        #        sample_mask_img = cv2.resize(sample_mask_img,(128,128))
        #        plt.imshow(sample_mask_img)
        #        sample_mask_img = np.reshape(sample_mask_img,[1,128,128,3])
        #        sample_mask_img = sample_mask_img/255.0
        #        # Predict the image read with our model
        #
        #        model.predict(sample_mask_img)
        #        model.save('masknet.h5')
        mask_label = {0:'MASK',1:'NO MASK'}
        dist_label = {0:(0,255,0),1:(255,0,0)}
        # If the no. of captured faces is more than 1, try finding distance between those faces using euclidean method
        # If distance between individuals is less than the threshold sound alert message
        # If the no. of captured faces is less than 2, display a message


        speak = False
        sound_file=''
        text_response =''
        if len(captured_faces)>=2:
            label = [0 for i in range(len(captured_faces))]
            mask = []
            nomask = []
            lists =[]


            # Euclidean distance calculation between faces
            for i in range(len(captured_faces)-1):
                for j in range(i+1, len(captured_faces)):
                    dist = distance.euclidean(captured_faces[i][:2],captured_faces[j][:2])
                    print(dist)
                    lists.append(dist)

                    if MIN_DISTANCE>dist:
                        label[i] = 1
                        label[j] = 1




            new_color_img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # Insert annotations in the image
            for i in range(len(captured_faces)):
                (x,y,w,h) = captured_faces[i]
                crop = new_color_img[y:y+h,x:x+w]
                crop = cv2.resize(crop,(128,128))
                crop = np.reshape(crop,[1,128,128,3])/255.0
                mask_result = model.predict(crop)
                cv2.putText(new_color_img,mask_label[mask_result.argmax()],(x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.5,dist_label[label[i]],2)
                if mask_label[mask_result.argmax()] == 'NO MASK':

                    nomask.append(lists[i])

                cv2.rectangle(new_color_img,(x,y),(x+w,y+h),dist_label[label[i]],1)
            plt.figure(figsize=(10,10))
            #plt.show(new_color_img)
            plt.savefig('/Users/shaivyachandra/PycharmProjects/wic/venv/maskornot.png')
            # Generate audio alert message if distance is lesser than minimum threshold
            mindist = min(nomask)
            if mindist<MIN_DISTANCE:
                text_response = 'Someone passing by without mask near you at '+str(round(mindist, 2))+'ft'
                tts = gTTS('Someone passing by without mask near you at '+str(round(mindist, 2))+'ft')
                tts.save('response.wav')
                print('saved')

                speak = True

        else:
            print("No. of faces detected is less than 2")
        #Audio(sound_file, autoplay=True)
        if speak:
            playsound('/Users/shaivyachandra/PycharmProjects/response.wav')
        return text_response


    def letsrun(self,image):
        face_model =self.loadfacemodel()
        faces,img = self.readimage(face_model)
        self.findfaces(faces,img)
        model = self.trainthedragon()
        model.save('visionmodel')

    def getresponse(self):
        model = keras.models.load_model('visionmodel')
        face_model =self.loadfacemodel()
        faces,img = self.readimage(face_model)
        return self.test(model,img,faces)




app = Flask(__name__)
api = Api(app)

class apicall(Resource):
    def __init__(self):

        # Create a request parser
        parser = reqparse.RequestParser()
        parser.add_argument("image", type=werkzeug.datastructures.FileStorage, location='files')
        # Sending more info in the form? simply add additional arguments, with the location being 'form'
        # parser.add_argument("other_arg", type=str, location='form')
        self.req_parser = parser



    def get(self):
        b1 = visionmaskdetector()
        responsestr =b1.getresponse()

        return {  'Response':responsestr
        }

    def post(self):

        # The image is retrieved as a file
        image_file = self.req_parser.parse_args(strict=True).get("image", None)
        print(image_file)
        if image_file:
            # Get the byte content using `.read()`
            image = image_file.read()
            # Now do something with the image...
            return "Yay, you sent an image!"
        else:
            return "No image sent :("

api.add_resource(apicall, '/')
if __name__ == '__main__':
    b1 = visionmaskdetector()
    #b1.letsrun('')
    app.run()
    #checkdir()
#    b1 = visionmaskdetector()
#    b1.letsrun('')



